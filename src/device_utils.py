# device_utils.py - è®¾å¤‡ç®¡ç†å·¥å…·
# äººè¯ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨GPUï¼Œç¡®ä¿è®­ç»ƒé€Ÿåº¦

import torch

def get_device():
    """
    è‡ªåŠ¨æ£€æµ‹å¹¶è¿”å›æœ€ä½³è®¾å¤‡
    
    è¿”å›:
        device: torch.deviceå¯¹è±¡
        è®¾å¤‡ä¿¡æ¯å­—ç¬¦ä¸²
    """
    if torch.cuda.is_available():
        device = torch.device('cuda')
        device_name = torch.cuda.get_device_name(0)
        device_count = torch.cuda.device_count()
        info = f"CUDA ({device_name})"
        if device_count > 1:
            info += f" - {device_count} GPUs available"
        return device, info
    else:
        device = torch.device('cpu')
        return device, "CPU"

def print_device_info():
    """æ‰“å°è®¾å¤‡ä¿¡æ¯"""
    device, info = get_device()
    
    print("=" * 70)
    print("ğŸ–¥ï¸  è®¾å¤‡ä¿¡æ¯")
    print("=" * 70)
    print(f"   ä½¿ç”¨è®¾å¤‡: {info}")
    
    if torch.cuda.is_available():
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"   GPU {i}: {props.name}")
            print(f"     æ€»å†…å­˜: {props.total_memory / 1024**3:.2f} GB")
            print(f"     è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        
        # æ˜¾ç¤ºå½“å‰GPUå†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
            memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
            print(f"   å½“å‰GPUå†…å­˜ä½¿ç”¨:")
            print(f"     å·²åˆ†é…: {memory_allocated:.2f} GB")
            print(f"     å·²ä¿ç•™: {memory_reserved:.2f} GB")
    else:
        print("   âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        print("   ğŸ’¡ æç¤ºï¼šå¦‚æœæœ‰NVIDIA GPUï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
    
    print("=" * 70 + "\n")
    
    return device

def verify_gpu_usage(model, sample_input):
    """
    éªŒè¯æ¨¡å‹å’Œæ•°æ®æ˜¯å¦åœ¨GPUä¸Š
    
    è¾“å…¥:
        model: æ¨¡å‹å¯¹è±¡
        sample_input: ç¤ºä¾‹è¾“å…¥tensor
    
    è¿”å›: æ˜¯å¦åœ¨GPUä¸Š
    """
    model_on_gpu = next(model.parameters()).is_cuda
    input_on_gpu = sample_input.is_cuda
    
    if model_on_gpu and input_on_gpu:
        print("âœ… éªŒè¯é€šè¿‡ï¼šæ¨¡å‹å’Œæ•°æ®éƒ½åœ¨GPUä¸Š")
        return True
    else:
        print("âš ï¸  è­¦å‘Šï¼š")
        if not model_on_gpu:
            print("   - æ¨¡å‹ä¸åœ¨GPUä¸Š")
        if not input_on_gpu:
            print("   - è¾“å…¥æ•°æ®ä¸åœ¨GPUä¸Š")
        return False

