# train.py - è®­ç»ƒ/æµ‹è¯•é€»è¾‘ï¼ˆåƒå¯¼æ¼”æ‹ç”µå½±ï¼‰
# äººè¯ï¼šä¸“æ³¨è®­ç»ƒæµç¨‹ï¼Œæ‰€æœ‰å¯è§†åŒ–éƒ½åœ¨è¿™é‡Œ

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from config.config import config, reverse_vocab
from scripts.data import get_data, show_sample  # ä½¿ç”¨æ™ºèƒ½æ•°æ®åŠ è½½å™¨
from scripts.inference import save_model  # æ¨¡å‹ä¿å­˜åŠŸèƒ½
from src.transformer import MiniTransformer
from src.mask_utils import create_decoder_mask, create_padding_mask
from src.device_utils import get_device, print_device_info, verify_gpu_usage

def train_model():
    """äººè¯ï¼šè®­ç»ƒæµç¨‹çš„æ€»æŒ‡æŒ¥"""
    # 0. æ£€æµ‹å¹¶è®¾ç½®è®¾å¤‡ï¼ˆGPU/CPUï¼‰
    device, device_info = get_device()
    print_device_info()
    
    # 1. å‡†å¤‡æ•°æ®ï¼ˆæ™ºèƒ½åŠ è½½ï¼šä¼˜å…ˆçœŸå®æ•°æ®ï¼Œfallbackåˆ°ç©å…·æ•°æ®ï¼‰
    enc_inputs, dec_inputs, targets, _ = get_data()
    
    # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š
    enc_inputs = enc_inputs.to(device)
    dec_inputs = dec_inputs.to(device)
    targets = targets.to(device)
    
    # æ˜¾ç¤ºè®­ç»ƒæ ·æœ¬ï¼ˆå¯è§†åŒ–åœ¨è®­ç»ƒæ¨¡å—ï¼‰
    sample_enc, sample_tgt = show_sample(enc_inputs[0], targets[0])
    print("\nğŸ“š ç©å…·ä»»åŠ¡: æŠŠå­—æ¯åºåˆ—ç¿»è¯‘æˆæ•°å­—åºåˆ—")
    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {sample_enc} -> {sample_tgt}")
    
    # 2. åˆ›å»ºæ¨¡å‹ï¼ˆåªä¾èµ–é…ç½®ï¼‰å¹¶ç§»åˆ°è®¾å¤‡ä¸Š
    model = MiniTransformer()
    model = model.to(device)
    
    # éªŒè¯GPUä½¿ç”¨
    verify_gpu_usage(model, enc_inputs)
    
    # 3. å®šä¹‰è®­ç»ƒç»„ä»¶ï¼ˆæŸå¤±/ä¼˜åŒ–å™¨ï¼‰
    criterion = nn.CrossEntropyLoss(ignore_index=config.VOCAB['<pad>'])
    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)
    
    # æ—©åœæœºåˆ¶å‚æ•°
    early_stop_patience = 50  # å¦‚æœ50ä¸ªepochæŸå¤±æ²¡æœ‰æ”¹å–„ï¼Œå°±åœæ­¢
    min_delta = 1e-4          # è‡³å°‘é™ä½è¿™ä¹ˆå¤šæ‰ç®—â€œçœŸæ­£è¿›æ­¥â€
    best_loss = float('inf')
    no_improve_count = 0
    best_model_state = None
    best_epoch = -1
    
    # 4. å‡†å¤‡æ‰¹å¤„ç†
    total_samples = enc_inputs.size(0)
    batch_size = config.BATCH_SIZE
    num_batches = (total_samples + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {config.EPOCHS} è½®")
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {total_samples} ä¸ªæ ·æœ¬, batch_size={batch_size}, {num_batches} ä¸ªbatch/epoch")
    print("ğŸ“ ä½¿ç”¨å®Œæ•´æ©ç ç­–ç•¥ï¼šlook-ahead mask + padding mask")
    print(f"ğŸ›‘ æ—©åœæœºåˆ¶ï¼šå¦‚æœ {early_stop_patience} ä¸ªepochæŸå¤±æ— æ”¹å–„ï¼Œå°†è‡ªåŠ¨åœæ­¢\n")
    
    for epoch in range(config.EPOCHS):
        epoch_loss = 0.0
        
        # æ¯ä¸ªepochæ‰“ä¹±æ•°æ®é¡ºåº
        indices = torch.randperm(total_samples)
        enc_inputs_shuffled = enc_inputs[indices]
        dec_inputs_shuffled = dec_inputs[indices]
        targets_shuffled = targets[indices]
        
        # æŒ‰batchå¤„ç†
        for batch_idx in range(num_batches):
            optimizer.zero_grad()
            
            # è·å–å½“å‰batchçš„æ•°æ®
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_samples)
            
            batch_enc_inputs = enc_inputs_shuffled[start_idx:end_idx].to(device)
            batch_dec_inputs = dec_inputs_shuffled[start_idx:end_idx].to(device)
            batch_targets = targets_shuffled[start_idx:end_idx].to(device)
            
            # åˆ›å»ºå®Œæ•´çš„æ©ç ï¼ˆæ¯ä¸ªbatchåŠ¨æ€åˆ›å»ºï¼Œæ”¯æŒä¸åŒé•¿åº¦ï¼‰
            # 1. Decoderæ©ç ï¼ˆlook-ahead + paddingï¼‰
            dec_mask = create_decoder_mask(batch_dec_inputs)  # [batch, dec_len, dec_len]
            
            # 2. Encoderæ©ç ï¼ˆencoderçš„paddingï¼‰- ç”¨äºEncoderè‡ªæ³¨æ„åŠ›
            enc_mask = create_padding_mask(batch_enc_inputs)  # [batch, 1, 1, enc_len]
            
            # å‰å‘ä¼ æ’­ï¼ˆä¼ å…¥æ‰€æœ‰æ©ç ï¼‰
            logits = model(
                batch_enc_inputs, 
                batch_dec_inputs, 
                look_ahead_mask=dec_mask,
                enc_padding_mask=enc_mask  # ç”¨äºEncoderè‡ªæ³¨æ„åŠ›å’ŒEncoder-Decoderæ³¨æ„åŠ›
            )
            
            # è®¡ç®—æŸå¤± (éœ€è¦reshape)
            loss = criterion(
                logits.view(-1, len(config.VOCAB)),
                batch_targets.view(-1)
            )
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        loss_value = epoch_loss / num_batches
        scheduler.step(loss_value)
        
        # æ—©åœæœºåˆ¶ï¼šæ£€æŸ¥æŸå¤±æ˜¯å¦æ”¹å–„
        if loss_value + min_delta < best_loss:
            best_loss = loss_value
            best_epoch = epoch
            no_improve_count = 0
            # æ·±æ‹·è´æƒé‡ï¼Œç¡®ä¿åç»­è®­ç»ƒä¸ä¼šè¦†ç›–æœ€ä½³çŠ¶æ€
            best_model_state = {
                k: v.detach().cpu().clone() for k, v in model.state_dict().items()
            }
        else:
            no_improve_count += 1
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        if epoch % 20 == 0 or epoch == config.EPOCHS - 1:
            lr = optimizer.param_groups[0]['lr']
            print(f"âœ… Epoch {epoch}/{config.EPOCHS} Loss: {loss_value:.4f} LR: {lr:.6f} (æœ€ä½³: {best_loss:.4f})")
        
        # æ—©åœæ£€æŸ¥
        if no_improve_count >= early_stop_patience:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼šæŸå¤±åœ¨ {early_stop_patience} ä¸ªepochå†…æ— æ”¹å–„")
            print(f"   æœ€ä½³æŸå¤±: {best_loss:.4f} (Epoch {best_epoch})")
            print(f"   å½“å‰æŸå¤±: {loss_value:.4f}")
            print(f"   å·²è®­ç»ƒ: {epoch + 1}/{config.EPOCHS} epochs")
            break
    
    # 5. æ¢å¤æœ€ä½³æ¨¡å‹å¹¶ä¿å­˜
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nğŸ’¾ å·²æ¢å¤æœ€ä½³æ¨¡å‹ (æŸå¤±: {best_loss:.4f} @ Epoch {best_epoch})")
    
    save_model(model, config.MODEL_SAVE_PATH)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {config.MODEL_SAVE_PATH}")
    
    return model

def test_model(model):
    """äººè¯ï¼šè€ƒè€ƒè®­ç»ƒå¥½çš„AI"""
    print("\nâœ… è®­ç»ƒå®Œæˆ! æ¥æµ‹è¯•ä¸€ä¸‹:")
    device = next(model.parameters()).device  # è·å–æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
    test_input = torch.tensor([[config.VOCAB['a'], config.VOCAB['b'], config.VOCAB['c']]], device=device)  # "a b c"
    
    # ç”Ÿæˆè¿‡ç¨‹: ä»<sos>å¼€å§‹ï¼Œä¸€æ­¥æ­¥é¢„æµ‹
    generated = [config.VOCAB['<sos>']]
    for i in range(3):  # ç”Ÿæˆ3ä¸ªæ•°å­—
        dec_input = torch.tensor([generated], device=device)
        with torch.no_grad():
            # ç”¨å½“å‰å·²ç”Ÿæˆçš„å†…å®¹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
            logits = model(test_input, dec_input)
            next_token = torch.argmax(logits[0, -1], dim=-1).item()
            generated.append(next_token)
    
    # è½¬æ¢ç»“æœ
    result = [reverse_vocab[token] for token in generated[1:]]  # è·³è¿‡<sos>
    print(f"\nğŸ¯ æµ‹è¯•ç»“æœ:")
    print(f"è¾“å…¥: [a, b, c]")
    print(f"è¾“å‡º: {result} (åº”è¯¥æ¥è¿‘ ['1','2','3'])")
    
    # =============== æ€»ç»“æ•´ä¸ªæµç¨‹ ===============
    print("\nğŸ§  ä»£ç æ ¸å¿ƒé€»è¾‘ (äººè¯ç‰ˆ):")
    print("1. Encoder (ç†è§£éƒ¨åˆ†):")
    print("   - ç”¨'å¤šå¤´æ³¨æ„åŠ›'è®©æ¯ä¸ªè¯å’Œå…¶ä»–è¯'å¯¹çœ¼ç¥' (æ¯”å¦‚'a'å’Œ'b'çš„å…³ç³»)")
    print("   - è¾“å‡ºå¯¹è¾“å…¥çš„'æ·±åº¦ç†è§£'")
    print("2. Decoder (ç”Ÿæˆéƒ¨åˆ†):")
    print("   - ç”¨'æ©ç å¤šå¤´æ³¨æ„åŠ›'åªèƒ½çœ‹å·²ç”Ÿæˆçš„è¯ (ç”Ÿæˆ'1'æ—¶çœ‹ä¸åˆ°'2')")
    print("   - ç”¨'Encoder-Decoderæ³¨æ„åŠ›'å‚è€ƒEncoderçš„ç†è§£ (æŠŠ'a'å¯¹åº”åˆ°'1')")
    print("3. å¤šå¤´æ³¨æ„åŠ› = è¯·å¤šä¸ªä¾¦æ¢å›¢é˜Ÿï¼Œä»ä¸åŒè§’åº¦åˆ†æå¥å­")
    print("   - ä¸€ä¸ªå›¢é˜Ÿçœ‹è¯­æ³•å…³ç³»ï¼Œä¸€ä¸ªå›¢é˜Ÿçœ‹è¯­ä¹‰å…³ç³»...")
    print("   - æœ€ååˆå¹¶æŠ¥å‘Šï¼Œå¾—åˆ°å…¨é¢ç†è§£")
    print("\nâœ¨ ä½ æˆåŠŸå®ç°äº†Transformerçš„æ ¸å¿ƒ! è¿™å°±æ˜¯ChatGPTçš„'å¿ƒè„'")

if __name__ == "__main__":
    trained_model = train_model()
    test_model(trained_model)

