# data.py - æ•°æ®å‡†å¤‡ï¼ˆåªåšä¸€ä»¶äº‹ï¼šæä¾›å¹²å‡€æ•°æ®ï¼‰
# äººè¯ï¼šä¸“æ³¨æ•°æ®ç”Ÿæˆï¼Œä¸å…³å¿ƒæ¨¡å‹æ€ä¹ˆç”¨

import sys
import os
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import torch
from typing import List, Tuple, Optional
from config.config import config, reverse_vocab  # åªä¾èµ–é…ç½®ï¼Œä¸ä¾èµ–æ¨¡å‹

def get_toy_data():
    """
    äººè¯ï¼šç”Ÿæˆç©å…·æ•°æ®ï¼Œä¸“æ³¨å±•ç¤ºTransformerèƒ½åŠ›
    è¿”å›: encoderè¾“å…¥, decoderè¾“å…¥, ç›®æ ‡, æ©ç 
    """
    vocab = config.VOCAB
    
    # è¾“å…¥: [batch_size, seq_len]
    encoder_inputs = torch.tensor([
        [vocab['a'], vocab['b'], vocab['c']],  # "a b c"
        [vocab['b'], vocab['a'], vocab['c']],  # "b a c"
    ])
    
    # è¾“å‡º: éœ€è¦åŠ <sos>å’Œ<eos> (å› ä¸ºæ˜¯è‡ªå›å½’ç”Ÿæˆ)
    decoder_inputs = torch.tensor([
        [vocab['<sos>'], vocab['1'], vocab['2'], vocab['3']],  # "<sos> 1 2 3"
        [vocab['<sos>'], vocab['2'], vocab['1'], vocab['3']],
    ])
    
    # ç›®æ ‡: é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ (æ‰€ä»¥æ¯”decoder_inputså°‘ä¸€ä¸ª<eos>)
    targets = torch.tensor([
        [vocab['1'], vocab['2'], vocab['3'], vocab['<eos>']],
        [vocab['2'], vocab['1'], vocab['3'], vocab['<eos>']],
    ])
    
    # åˆ›å»ºæ©ç : Decoderä¸èƒ½çœ‹æœªæ¥è¯ (æ¯”å¦‚ç”Ÿæˆ"1"æ—¶ä¸èƒ½çœ‹"2")
    look_ahead_mask = torch.tril(torch.ones(4, 4))  # 4=åºåˆ—é•¿åº¦
    
    return encoder_inputs, decoder_inputs, targets, look_ahead_mask

def show_sample(enc_input, target):
    """
    äººè¯ï¼šæŠŠæ•°å­—IDå˜å›æ–‡å­—ï¼Œè®©äººçœ‹æ‡‚
    è¾“å…¥: encoderè¾“å…¥tensor, target tensor
    è¿”å›: (è¾“å…¥æ–‡å­—åˆ—è¡¨, ç›®æ ‡æ–‡å­—åˆ—è¡¨)
    """
    vocab = config.VOCAB
    enc_text = [reverse_vocab[i.item()] for i in enc_input]
    target_text = [reverse_vocab[i.item()] for i in target if i.item() != vocab['<eos>']]
    return enc_text, target_text

# ==================== çœŸå®æ•°æ®åŠ è½½ ====================

def load_data_from_file(file_path: str, separator: str = "\t") -> List[Tuple[str, str]]:
    """
    äººè¯ï¼šä»æ–‡ä»¶è¯»å–æ•°æ®å¯¹ï¼ˆè¾“å…¥åºåˆ—ï¼Œè¾“å‡ºåºåˆ—ï¼‰
    æ–‡ä»¶æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼Œç”¨åˆ†éš”ç¬¦åˆ†å¼€è¾“å…¥å’Œè¾“å‡º
    ä¾‹å¦‚ï¼ša b c\t1 2 3
    
    è¿”å›: [(è¾“å…¥åºåˆ—, è¾“å‡ºåºåˆ—), ...]
    """
    data_pairs = []
    if not os.path.exists(file_path):
        print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return data_pairs
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:  # è·³è¿‡ç©ºè¡Œ
                continue
            if line.startswith('#'):  # è·³è¿‡æ³¨é‡Šè¡Œ
                continue
            parts = line.split(separator)
            if len(parts) == 2:
                input_seq = parts[0].strip()
                output_seq = parts[1].strip()
                data_pairs.append((input_seq, output_seq))
            else:
                print(f"âš ï¸  è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ: {line}")
    
    print(f"âœ… ä»æ–‡ä»¶åŠ è½½äº† {len(data_pairs)} ä¸ªæ ·æœ¬")
    return data_pairs

def tokenize_sequence(sequence: str) -> List[str]:
    """
    äººè¯ï¼šæŠŠå­—ç¬¦ä¸²åºåˆ—åˆ†å‰²æˆtokenåˆ—è¡¨
    ä¾‹å¦‚: "a b c" -> ["a", "b", "c"]
    """
    return sequence.split()

def sequence_to_ids(sequence: List[str], vocab: dict, max_len: Optional[int] = None) -> List[int]:
    """
    äººè¯ï¼šæŠŠtokenåˆ—è¡¨è½¬æ¢æˆIDåˆ—è¡¨
    å¦‚æœtokenä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œä½¿ç”¨<unk>
    å¦‚æœæŒ‡å®šmax_lenï¼Œä¼šè¿›è¡Œpaddingæˆ–æˆªæ–­
    """
    ids = []
    for token in sequence:
        if token in vocab:
            ids.append(vocab[token])
        else:
            ids.append(vocab.get('<unk>', 9))  # æœªçŸ¥è¯
    
    # Paddingæˆ–æˆªæ–­
    if max_len is not None:
        if len(ids) < max_len:
            ids.extend([vocab['<pad>']] * (max_len - len(ids)))
        else:
            ids = ids[:max_len]
    
    return ids

def get_real_data(file_path: Optional[str] = None):
    """
    äººè¯ï¼šä»çœŸå®æ–‡ä»¶åŠ è½½æ•°æ®å¹¶è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
    è¿”å›: encoderè¾“å…¥, decoderè¾“å…¥, ç›®æ ‡, æ©ç 
    
    å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–æœªæŒ‡å®šï¼Œè¿”å›Noneï¼ˆå¯ä»¥fallbackåˆ°ç©å…·æ•°æ®ï¼‰
    """
    file_path = file_path or config.DATA_FILE
    if file_path is None or not os.path.exists(file_path):
        return None
    
    # 1. ä»æ–‡ä»¶åŠ è½½åŸå§‹æ•°æ®
    data_pairs = load_data_from_file(file_path, config.DATA_SEPARATOR)
    if not data_pairs:
        return None
    
    vocab = config.VOCAB
    
    # 2. æ‰¾åˆ°æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆç”¨äºpaddingï¼‰
    max_enc_len = max(len(tokenize_sequence(pair[0])) for pair in data_pairs)
    max_dec_len = max(len(tokenize_sequence(pair[1])) for pair in data_pairs) + 1  # +1 for <eos>
    
    # 3. è½¬æ¢ä¸ºIDåºåˆ—
    encoder_inputs_list = []
    decoder_inputs_list = []
    targets_list = []
    
    for input_seq, output_seq in data_pairs:
        # è¾“å…¥åºåˆ—
        input_tokens = tokenize_sequence(input_seq)
        enc_ids = sequence_to_ids(input_tokens, vocab, max_enc_len)
        encoder_inputs_list.append(enc_ids)
        
        # è¾“å‡ºåºåˆ—ï¼ˆéœ€è¦åŠ <sos>å’Œ<eos>ï¼‰
        output_tokens = tokenize_sequence(output_seq)
        output_ids = sequence_to_ids(output_tokens, vocab)
        
        # decoderè¾“å…¥: <sos> + è¾“å‡ºåºåˆ—
        dec_input_ids = [vocab['<sos>']] + output_ids
        if len(dec_input_ids) < max_dec_len:
            dec_input_ids.extend([vocab['<pad>']] * (max_dec_len - len(dec_input_ids)))
        decoder_inputs_list.append(dec_input_ids)
        
        # ç›®æ ‡: è¾“å‡ºåºåˆ— + <eos>
        target_ids = output_ids + [vocab['<eos>']]
        if len(target_ids) < max_dec_len:
            target_ids.extend([vocab['<pad>']] * (max_dec_len - len(target_ids)))
        targets_list.append(target_ids)
    
    # 4. è½¬æ¢ä¸ºtensor
    encoder_inputs = torch.tensor(encoder_inputs_list, dtype=torch.long)
    decoder_inputs = torch.tensor(decoder_inputs_list, dtype=torch.long)
    targets = torch.tensor(targets_list, dtype=torch.long)
    
    # 5. åˆ›å»ºæ©ç 
    # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯åŸºç¡€look-ahead maskï¼Œå®Œæ•´çš„æ©ç åœ¨è®­ç»ƒæ—¶åˆ›å»º
    # å› ä¸ºéœ€è¦batchä¿¡æ¯ï¼Œæ‰€ä»¥åœ¨è¿™é‡Œåªè¿”å›å½¢çŠ¶æ¨¡æ¿
    look_ahead_mask_template = torch.tril(torch.ones(max_dec_len, max_dec_len))
    
    return encoder_inputs, decoder_inputs, targets, look_ahead_mask_template

def get_data():
    """
    äººè¯ï¼šæ™ºèƒ½æ•°æ®åŠ è½½å™¨
    ä¼˜å…ˆå°è¯•ä»æ–‡ä»¶åŠ è½½çœŸå®æ•°æ®ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç©å…·æ•°æ®
    è¿”å›: encoderè¾“å…¥, decoderè¾“å…¥, ç›®æ ‡, æ©ç 
    """
    # å°è¯•åŠ è½½çœŸå®æ•°æ®
    real_data = get_real_data()
    if real_data is not None:
        print("ğŸ“‚ ä½¿ç”¨çœŸå®æ•°æ®æ–‡ä»¶")
        return real_data
    
    # Fallbackåˆ°ç©å…·æ•°æ®
    print("ğŸ® ä½¿ç”¨ç©å…·æ•°æ®ï¼ˆæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼‰")
    return get_toy_data()

