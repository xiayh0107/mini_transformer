# evaluate.py - æ¨¡å‹è¯„ä¼°æ¨¡å—
# äººè¯ï¼šå…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæ‰¾å‡ºé—®é¢˜ï¼Œæ˜ç¡®ä¼˜åŒ–æ–¹å‘

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import torch
import numpy as np
from typing import List, Tuple, Dict, Optional

from config.config import config
from scripts.data import load_data_from_file, tokenize_sequence, sequence_to_ids
from scripts.inference import load_model, predict, text_to_ids
from src.device_utils import print_device_info, verify_gpu_usage
from src.mask_utils import create_decoder_mask, create_padding_mask

class ModelEvaluator:
    """
    äººè¯ï¼šæ¨¡å‹è¯„ä¼°å™¨
    å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡ã€å›°æƒ‘åº¦ã€ç”Ÿæˆè´¨é‡ç­‰
    """
    
    def __init__(self, model_path: Optional[str] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        è¾“å…¥:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        print("ğŸ“Š åˆå§‹åŒ–æ¨¡å‹è¯„ä¼°å™¨...")
        resolved_path = model_path or config.MODEL_SAVE_PATH
        self.model = load_model(resolved_path)
        if self.model is None:
            raise ValueError("æ¨¡å‹åŠ è½½å¤±è´¥ï¼")
        
        # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
        print_device_info()
        
        # éªŒè¯GPUä½¿ç”¨
        device = next(self.model.parameters()).device
        sample_input = torch.tensor([[0]], device=device)  # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        verify_gpu_usage(self.model, sample_input)
        
        self.vocab = config.VOCAB
        self.model.eval()
        print("âœ… è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ\n")

    @staticmethod
    def _repetition_metrics(tokens: List[str]) -> Tuple[float, int]:
        if not tokens:
            return 0.0, 0

        unique_tokens = len(set(tokens))
        total_tokens = len(tokens)
        repetition_rate = 1.0 - (unique_tokens / total_tokens)

        max_consecutive = 1
        current = 1
        for i in range(1, total_tokens):
            if tokens[i] == tokens[i - 1]:
                current += 1
                if current > max_consecutive:
                    max_consecutive = current
            else:
                current = 1

        return repetition_rate, max_consecutive
    
    def load_test_data(self, file_path: str = None) -> List[Tuple[str, str]]:
        """
        åŠ è½½æµ‹è¯•æ•°æ®
        
        è¾“å…¥:
            file_path: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼‰
        
        è¿”å›: [(è¾“å…¥åºåˆ—, ç›®æ ‡åºåˆ—), ...]
        """
        file_path = file_path or config.DATA_FILE
        if file_path is None or not file_path:
            print("âš ï¸  æœªæŒ‡å®šæµ‹è¯•æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®")
            file_path = config.DATA_FILE
        
        data_pairs = load_data_from_file(file_path, config.DATA_SEPARATOR)
        if not data_pairs:
            print("âš ï¸  æµ‹è¯•æ•°æ®ä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤æµ‹è¯•é›†")
            # åˆ›å»ºé»˜è®¤æµ‹è¯•é›†
            data_pairs = [
                ("a b c", "3 1 2"),
                ("b a c", "1 3 2"),
                ("c a b", "2 3 1"),
                ("a c b", "3 2 1"),
                ("b c a", "1 2 3"),
                ("c b a", "2 1 3"),
                ("c b", "2 1"),
                ("a", "3"),
                ("b", "1"),
                ("c", "2"),
            ]
        
        return data_pairs
    
    def exact_match_accuracy(self, test_data: List[Tuple[str, str]]) -> float:
        """
        å®Œå…¨åŒ¹é…å‡†ç¡®ç‡
        
        è¾“å…¥:
            test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨
        
        è¿”å›: å‡†ç¡®ç‡ (0-1)
        """
        correct = 0
        total = len(test_data)
        
        for input_seq, target_seq in test_data:
            predicted = predict(self.model, input_seq, max_length=20)
            # æ ‡å‡†åŒ–ï¼šå»é™¤å¤šä½™ç©ºæ ¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨æ¯”è¾ƒ
            pred_tokens = predicted.split()
            target_tokens = target_seq.split()
            
            if pred_tokens == target_tokens:
                correct += 1
        
        accuracy = correct / total if total > 0 else 0.0
        return accuracy
    
    def token_accuracy(self, test_data: List[Tuple[str, str]]) -> Tuple[float, Dict]:
        """
        Tokençº§åˆ«çš„å‡†ç¡®ç‡
        
        è¾“å…¥:
            test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨
        
        è¿”å›: (å‡†ç¡®ç‡, è¯¦ç»†ç»Ÿè®¡)
        """
        correct_tokens = 0
        total_tokens = 0
        position_errors = []  # æ¯ä¸ªä½ç½®çš„é”™è¯¯ç»Ÿè®¡
        
        for input_seq, target_seq in test_data:
            predicted = predict(self.model, input_seq, max_length=20)
            pred_tokens = predicted.split()
            target_tokens = target_seq.split()
            
            # æŒ‰ä½ç½®æ¯”è¾ƒ
            max_len = max(len(pred_tokens), len(target_tokens))
            for i in range(max_len):
                if i >= len(position_errors):
                    position_errors.append({"correct": 0, "total": 0, "errors": []})
                
                entry = position_errors[i]
                entry["total"] += 1
                
                if i < len(pred_tokens) and i < len(target_tokens):
                    if pred_tokens[i] == target_tokens[i]:
                        correct_tokens += 1
                        entry["correct"] += 1
                    else:
                        entry["errors"].append({
                            "input": input_seq,
                            "expected": target_tokens[i],
                            "got": pred_tokens[i],
                            "position": i
                        })
                elif i < len(target_tokens):
                    # é¢„æµ‹å¤ªçŸ­
                    entry["errors"].append({
                        "input": input_seq,
                        "expected": target_tokens[i],
                        "got": "<MISSING>",
                        "position": i
                    })
                else:
                    # é¢„æµ‹å¤ªé•¿
                    entry["errors"].append({
                        "input": input_seq,
                        "expected": "<EOS>",
                        "got": pred_tokens[i],
                        "position": i
                    })
                
                if i < len(pred_tokens) and i < len(target_tokens):
                    total_tokens += 1
        
        accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0
        return accuracy, {"position_errors": position_errors}
    
    def sequence_length_accuracy(self, test_data: List[Tuple[str, str]]) -> Dict:
        """
        åºåˆ—é•¿åº¦å‡†ç¡®æ€§åˆ†æ
        
        è¾“å…¥:
            test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨
        
        è¿”å›: é•¿åº¦ç»Ÿè®¡ä¿¡æ¯
        """
        length_stats = {
            "exact_match": 0,
            "too_short": 0,
            "too_long": 0,
            "avg_pred_len": 0,
            "avg_target_len": 0,
            "length_errors": []
        }
        
        total = len(test_data)
        pred_lengths = []
        target_lengths = []
        
        for input_seq, target_seq in test_data:
            predicted = predict(self.model, input_seq, max_length=20)
            pred_tokens = predicted.split()
            target_tokens = target_seq.split()
            
            pred_len = len(pred_tokens)
            target_len = len(target_tokens)
            
            pred_lengths.append(pred_len)
            target_lengths.append(target_len)
            
            if pred_len == target_len:
                length_stats["exact_match"] += 1
            elif pred_len < target_len:
                length_stats["too_short"] += 1
                length_stats["length_errors"].append({
                    "input": input_seq,
                    "target_len": target_len,
                    "pred_len": pred_len,
                    "type": "too_short"
                })
            else:
                length_stats["too_long"] += 1
                length_stats["length_errors"].append({
                    "input": input_seq,
                    "target_len": target_len,
                    "pred_len": pred_len,
                    "type": "too_long"
                })
        
        length_stats["exact_match"] /= total if total > 0 else 1
        length_stats["too_short"] /= total if total > 0 else 1
        length_stats["too_long"] /= total if total > 0 else 1
        length_stats["avg_pred_len"] = np.mean(pred_lengths) if pred_lengths else 0
        length_stats["avg_target_len"] = np.mean(target_lengths) if target_lengths else 0
        
        return length_stats
    
    def repetition_analysis(self, test_data: List[Tuple[str, str]]) -> Dict:
        """
        é‡å¤ç‡åˆ†æ
        
        è¾“å…¥:
            test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨
        
        è¿”å›: é‡å¤ç»Ÿè®¡ä¿¡æ¯
        """
        repetition_stats = {
            "avg_repetition_rate": 0.0,
            "avg_target_repetition_rate": 0.0,
            "avg_repetition_gap": 0.0,
            "max_repetition": 0,
            "max_target_repetition": 0,
            "repetition_examples": [],
        }
        
        total_pred_rate = 0.0
        total_target_rate = 0.0
        total_gap = 0.0
        
        for input_seq, target_seq in test_data:
            predicted = predict(self.model, input_seq, max_length=20)
            pred_tokens = predicted.split()
            target_tokens = target_seq.split()

            pred_rate, pred_max = self._repetition_metrics(pred_tokens)
            target_rate, target_max = self._repetition_metrics(target_tokens)

            rate_gap = max(0.0, pred_rate - target_rate)
            max_gap = max(0, pred_max - target_max)

            total_pred_rate += pred_rate
            total_target_rate += target_rate
            total_gap += rate_gap

            repetition_stats["max_repetition"] = max(repetition_stats["max_repetition"], pred_max)
            repetition_stats["max_target_repetition"] = max(repetition_stats["max_target_repetition"], target_max)

            if rate_gap > 0.15 or max_gap > 1:
                repetition_stats["repetition_examples"].append({
                    "input": input_seq,
                    "target": target_seq,
                    "predicted": predicted,
                    "pred_repetition_rate": pred_rate,
                    "target_repetition_rate": target_rate,
                    "repetition_rate_gap": rate_gap,
                    "max_consecutive": pred_max,
                    "target_max_consecutive": target_max,
                    "max_consecutive_gap": max_gap,
                })
        
        sample_count = len(test_data) if test_data else 1
        repetition_stats["avg_repetition_rate"] = total_pred_rate / sample_count
        repetition_stats["avg_target_repetition_rate"] = total_target_rate / sample_count
        repetition_stats["avg_repetition_gap"] = total_gap / sample_count
        
        return repetition_stats
    
    def perplexity(self, test_data: List[Tuple[str, str]]) -> float:
        """
        è®¡ç®—å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
        
        è¾“å…¥:
            test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨
        
        è¿”å›: å›°æƒ‘åº¦å€¼
        """
        total_loss = 0.0
        total_tokens = 0
        
        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.vocab['<pad>'], reduction='sum')
        
        # è·å–æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
        device = next(self.model.parameters()).device
        
        with torch.no_grad():
            for input_seq, target_seq in test_data:
                # å‡†å¤‡è¾“å…¥
                input_ids = text_to_ids(input_seq, self.vocab)
                if not input_ids:
                    continue
                
                encoder_input = torch.tensor([input_ids], dtype=torch.long, device=device)
                
                # å‡†å¤‡ç›®æ ‡
                target_tokens = tokenize_sequence(target_seq)
                target_ids = sequence_to_ids(target_tokens, self.vocab)
                target_ids.append(self.vocab['<eos>'])
                target_tensor = torch.tensor([target_ids], dtype=torch.long, device=device)
                
                # ç”Ÿæˆè¿‡ç¨‹å¹¶è®¡ç®—æŸå¤±
                generated = [self.vocab['<sos>']]
                batch_loss = 0.0
                batch_tokens = 0
                
                for step in range(len(target_ids) + 5):  # æœ€å¤šå¤šç”Ÿæˆ5ä¸ªtoken
                    dec_input = torch.tensor([generated], dtype=torch.long, device=device)
                    
                    # åˆ›å»ºæ©ç 
                    dec_mask = create_decoder_mask(dec_input)
                    enc_mask = create_padding_mask(encoder_input)
                    
                    # æ¨¡å‹é¢„æµ‹
                    logits = self.model(encoder_input, dec_input, 
                                      look_ahead_mask=dec_mask, 
                                      enc_padding_mask=enc_mask)
                    
                    # è®¡ç®—è¿™ä¸€æ­¥çš„æŸå¤±
                    if step < len(target_ids):
                        target_token = target_tensor[0, step].unsqueeze(0)
                        step_loss = criterion(logits[0, -1:], target_token)
                        batch_loss += step_loss.item()
                        batch_tokens += 1
                    
                    # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
                    next_token = torch.argmax(logits[0, -1], dim=-1).item()
                    if next_token == self.vocab['<eos>']:
                        break
                    generated.append(next_token)
                
                total_loss += batch_loss
                total_tokens += batch_tokens
        
        avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')
        perplexity = np.exp(avg_loss) if avg_loss != float('inf') else float('inf')
        
        return perplexity
    
    def error_analysis(self, test_data: List[Tuple[str, str]]) -> Dict:
        """
        é”™è¯¯åˆ†æ
        
        è¾“å…¥:
            test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨
        
        è¿”å›: é”™è¯¯ç»Ÿè®¡ä¿¡æ¯
        """
        error_stats = {
            "total_errors": 0,
            "mapping_errors": [],  # æ˜ å°„é”™è¯¯ï¼ˆå¦‚aåº”è¯¥->3ä½†->1ï¼‰
            "length_errors": [],
            "repetition_errors": [],
            "all_errors": []
        }
        
        for input_seq, target_seq in test_data:
            predicted = predict(self.model, input_seq, max_length=20)
            pred_tokens = predicted.split()
            target_tokens = target_seq.split()
            
            # æ£€æŸ¥æ˜ å°„é”™è¯¯
            input_tokens = tokenize_sequence(input_seq)
            min_len = min(len(input_tokens), len(target_tokens), len(pred_tokens))
            
            for i in range(min_len):
                expected_mapping = f"{input_tokens[i]} -> {target_tokens[i]}"
                actual_mapping = f"{input_tokens[i]} -> {pred_tokens[i] if i < len(pred_tokens) else '<MISSING>'}"
                
                if i < len(pred_tokens) and pred_tokens[i] != target_tokens[i]:
                    error_stats["mapping_errors"].append({
                        "input": input_seq,
                        "position": i,
                        "input_token": input_tokens[i],
                        "expected": target_tokens[i],
                        "got": pred_tokens[i],
                        "mapping": expected_mapping
                    })
                    error_stats["total_errors"] += 1
            
            # è®°å½•æ‰€æœ‰é”™è¯¯
            if pred_tokens != target_tokens:
                error_stats["all_errors"].append({
                    "input": input_seq,
                    "expected": target_seq,
                    "got": predicted
                })
        
        return error_stats
    
    def comprehensive_evaluation(self, test_data: List[Tuple[str, str]] = None) -> Dict:
        """
        ç»¼åˆè¯„ä¼°
        
        è¾“å…¥:
            test_data: æµ‹è¯•æ•°æ®ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨åŠ è½½ï¼‰
        
        è¿”å›: å®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š
        """
        if test_data is None:
            test_data = self.load_test_data()
        
        print("=" * 70)
        print("ğŸ“Š å¼€å§‹å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        print("=" * 70 + "\n")
        
        results = {}
        
        # 1. å®Œå…¨åŒ¹é…å‡†ç¡®ç‡
        print("1ï¸âƒ£  è®¡ç®—å®Œå…¨åŒ¹é…å‡†ç¡®ç‡...")
        results["exact_match_accuracy"] = self.exact_match_accuracy(test_data)
        print(f"   âœ… å®Œå…¨åŒ¹é…å‡†ç¡®ç‡: {results['exact_match_accuracy']:.2%}\n")
        
        # 2. Tokençº§åˆ«å‡†ç¡®ç‡
        print("2ï¸âƒ£  è®¡ç®—Tokençº§åˆ«å‡†ç¡®ç‡...")
        results["token_accuracy"], token_details = self.token_accuracy(test_data)
        print(f"   âœ… Tokenå‡†ç¡®ç‡: {results['token_accuracy']:.2%}\n")
        
        # 3. åºåˆ—é•¿åº¦åˆ†æ
        print("3ï¸âƒ£  åˆ†æåºåˆ—é•¿åº¦å‡†ç¡®æ€§...")
        results["length_stats"] = self.sequence_length_accuracy(test_data)
        print(f"   âœ… é•¿åº¦å®Œå…¨åŒ¹é…ç‡: {results['length_stats']['exact_match']:.2%}")
        print(f"   âœ… å¹³å‡é¢„æµ‹é•¿åº¦: {results['length_stats']['avg_pred_len']:.2f}")
        print(f"   âœ… å¹³å‡ç›®æ ‡é•¿åº¦: {results['length_stats']['avg_target_len']:.2f}\n")
        
        # 4. é‡å¤ç‡åˆ†æ
        print("4ï¸âƒ£  åˆ†æé‡å¤ç‡...")
        results["repetition_stats"] = self.repetition_analysis(test_data)
        print(f"   âœ… å¹³å‡é‡å¤ç‡: {results['repetition_stats']['avg_repetition_rate']:.2%}")
        print(f"   âœ… æœ€å¤§è¿ç»­é‡å¤: {results['repetition_stats']['max_repetition']}\n")
        
        # 5. å›°æƒ‘åº¦
        print("5ï¸âƒ£  è®¡ç®—å›°æƒ‘åº¦...")
        try:
            results["perplexity"] = self.perplexity(test_data)
            print(f"   âœ… å›°æƒ‘åº¦: {results['perplexity']:.2f}\n")
        except Exception as e:
            print(f"   âš ï¸  å›°æƒ‘åº¦è®¡ç®—å¤±è´¥: {e}\n")
            results["perplexity"] = None
        
        # 6. é”™è¯¯åˆ†æ
        print("6ï¸âƒ£  è¿›è¡Œé”™è¯¯åˆ†æ...")
        results["error_stats"] = self.error_analysis(test_data)
        print(f"   âœ… æ€»é”™è¯¯æ•°: {results['error_stats']['total_errors']}")
        print(f"   âœ… æ˜ å°„é”™è¯¯æ•°: {len(results['error_stats']['mapping_errors'])}\n")
        
        results["test_data"] = test_data
        results["token_details"] = token_details
        
        return results
    
    def print_detailed_report(self, results: Dict):
        """
        æ‰“å°è¯¦ç»†è¯„ä¼°æŠ¥å‘Š
        
        è¾“å…¥:
            results: è¯„ä¼°ç»“æœå­—å…¸
        """
        print("\n" + "=" * 70)
        print("ğŸ“‹ è¯¦ç»†è¯„ä¼°æŠ¥å‘Š")
        print("=" * 70 + "\n")
        
        # æ€»ä½“æŒ‡æ ‡
        print("ğŸ“Š æ€»ä½“æ€§èƒ½æŒ‡æ ‡:")
        print(f"   å®Œå…¨åŒ¹é…å‡†ç¡®ç‡: {results['exact_match_accuracy']:.2%}")
        print(f"   Tokenå‡†ç¡®ç‡: {results['token_accuracy']:.2%}")
        if results.get('perplexity') is not None:
            print(f"   å›°æƒ‘åº¦: {results['perplexity']:.2f}")
        print()
        
        # é•¿åº¦åˆ†æ
        print("ğŸ“ åºåˆ—é•¿åº¦åˆ†æ:")
        length_stats = results['length_stats']
        print(f"   é•¿åº¦å®Œå…¨åŒ¹é…ç‡: {length_stats['exact_match']:.2%}")
        print(f"   é¢„æµ‹è¿‡çŸ­ç‡: {length_stats['too_short']:.2%}")
        print(f"   é¢„æµ‹è¿‡é•¿ç‡: {length_stats['too_long']:.2%}")
        print(f"   å¹³å‡é¢„æµ‹é•¿åº¦: {length_stats['avg_pred_len']:.2f}")
        print(f"   å¹³å‡ç›®æ ‡é•¿åº¦: {length_stats['avg_target_len']:.2f}")
        print()
        
        # é‡å¤åˆ†æ
        print("ğŸ”„ é‡å¤ç‡åˆ†æ:")
        rep_stats = results['repetition_stats']
        print(f"   å¹³å‡é‡å¤ç‡(é¢„æµ‹): {rep_stats['avg_repetition_rate']:.2%}")
        print(f"   å¹³å‡é‡å¤ç‡(ç›®æ ‡): {rep_stats['avg_target_repetition_rate']:.2%}")
        print(f"   å¹³å‡é‡å¤ç‡å·®å€¼: {rep_stats['avg_repetition_gap']:.2%}")
        print(f"   æœ€å¤§è¿ç»­é‡å¤(é¢„æµ‹): {rep_stats['max_repetition']}")
        print(f"   æœ€å¤§è¿ç»­é‡å¤(ç›®æ ‡): {rep_stats['max_target_repetition']}")
        if rep_stats['repetition_examples']:
            print(f"   âš ï¸  å‘ç° {len(rep_stats['repetition_examples'])} ä¸ªé«˜é‡å¤ç‡æ ·æœ¬:")
            for ex in rep_stats['repetition_examples'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"      - è¾“å…¥: {ex['input']}")
                print(f"        ç›®æ ‡: {ex['target']}")
                print(f"        è¾“å‡º: {ex['predicted']}")
                print(
                    f"        é‡å¤ç‡å·®å€¼: {ex['repetition_rate_gap']:.2%}, "
                    f"æœ€å¤§è¿ç»­å·®å€¼: {ex['max_consecutive_gap']}"
                )
        print()
        
        # é”™è¯¯åˆ†æ
        print("âŒ é”™è¯¯åˆ†æ:")
        error_stats = results['error_stats']
        print(f"   æ€»é”™è¯¯æ•°: {error_stats['total_errors']}")
        print(f"   æ˜ å°„é”™è¯¯æ•°: {len(error_stats['mapping_errors'])}")
        
        if error_stats['mapping_errors']:
            print(f"   âš ï¸  æ˜ å°„é”™è¯¯ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
            for err in error_stats['mapping_errors'][:5]:
                print(f"      - ä½ç½® {err['position']}: {err['mapping']}")
                print(f"        å®é™…è¾“å‡º: {err['got']}")
        
        if error_stats['all_errors']:
            print(f"\n   âš ï¸  æ‰€æœ‰é”™è¯¯æ ·æœ¬ï¼ˆå‰5ä¸ªï¼‰:")
            for err in error_stats['all_errors'][:5]:
                print(f"      - è¾“å…¥: {err['input']}")
                print(f"        æœŸæœ›: {err['expected']}")
                print(f"        å®é™…: {err['got']}")
        print()
        
        # ä¼˜åŒ–å»ºè®®
        print("=" * 70)
        print("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
        print("=" * 70 + "\n")
        
        suggestions = []
        
        if results['exact_match_accuracy'] < 0.5:
            suggestions.append("ğŸ”´ å®Œå…¨åŒ¹é…å‡†ç¡®ç‡è¿‡ä½ï¼Œæ¨¡å‹åŸºæœ¬æ— æ³•æ­£ç¡®å®Œæˆä»»åŠ¡")
        
        if results['token_accuracy'] < 0.7:
            suggestions.append("ğŸŸ¡ Tokenå‡†ç¡®ç‡åä½ï¼Œéœ€è¦æ”¹è¿›æ¨¡å‹å­¦ä¹ èƒ½åŠ›")
        
        if length_stats['too_long'] > 0.3:
            suggestions.append("ğŸŸ¡ æ¨¡å‹ç”Ÿæˆè¿‡é•¿ï¼Œå¯èƒ½æ˜¯åœæ­¢æ¡ä»¶ä¸å½“æˆ–è®­ç»ƒä¸è¶³")
        
        if length_stats['too_short'] > 0.3:
            suggestions.append("ğŸŸ¡ æ¨¡å‹ç”Ÿæˆè¿‡çŸ­ï¼Œå¯èƒ½è¿‡æ—©åœæ­¢æˆ–å­¦ä¹ ä¸å……åˆ†")
        
        if rep_stats['avg_repetition_gap'] > 0.15:
            suggestions.append("ğŸ”´ é‡å¤ç‡è¿‡é«˜ï¼Œæ¨¡å‹é™·å…¥é‡å¤ç”Ÿæˆï¼Œéœ€è¦:")
            suggestions.append("   - æ£€æŸ¥è®­ç»ƒæ•°æ®æ˜¯å¦å¹³è¡¡")
            suggestions.append("   - å¢åŠ è®­ç»ƒè½®æ•°")
            suggestions.append("   - è°ƒæ•´å­¦ä¹ ç‡æˆ–ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦")
            suggestions.append("   - æ£€æŸ¥è§£ç ç­–ç•¥ï¼ˆè€ƒè™‘ä½¿ç”¨temperatureæˆ–beam searchï¼‰")
        
        if rep_stats['max_repetition'] - rep_stats['max_target_repetition'] > 1:
            suggestions.append("ğŸ”´ å­˜åœ¨ä¸¥é‡è¿ç»­é‡å¤é—®é¢˜ï¼Œæ¨¡å‹å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜")
        
        if len(error_stats['mapping_errors']) > len(error_stats['all_errors']) * 0.5:
            suggestions.append("ğŸŸ¡ æ˜ å°„é”™è¯¯è¾ƒå¤šï¼Œæ¨¡å‹å¯èƒ½:")
            suggestions.append("   - è®­ç»ƒæ•°æ®ä¸è¶³æˆ–è´¨é‡ä¸é«˜")
            suggestions.append("   - æ¨¡å‹å®¹é‡ä¸è¶³ï¼ˆd_modelæˆ–num_headså¤ªå°ï¼‰")
            suggestions.append("   - éœ€è¦æ›´å¤šè®­ç»ƒè½®æ•°")
        
        if results.get('perplexity') is not None and results['perplexity'] > 10:
            suggestions.append("ğŸŸ¡ å›°æƒ‘åº¦è¾ƒé«˜ï¼Œæ¨¡å‹ä¸ç¡®å®šæ€§å¤§ï¼Œéœ€è¦:")
            suggestions.append("   - å¢åŠ è®­ç»ƒæ•°æ®")
            suggestions.append("   - è°ƒæ•´æ¨¡å‹æ¶æ„")
            suggestions.append("   - æ”¹è¿›è®­ç»ƒç­–ç•¥")
        
        if not suggestions:
            suggestions.append("âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥å°è¯•:")
            suggestions.append("   - å¢åŠ è®­ç»ƒæ•°æ®ä»¥æé«˜æ³›åŒ–èƒ½åŠ›")
            suggestions.append("   - å°è¯•æ›´å¤æ‚çš„ä»»åŠ¡")
        
        for suggestion in suggestions:
            print(f"   {suggestion}")
        
        print("\n" + "=" * 70)


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´è¯„ä¼°"""
    import sys
    
    model_path = sys.argv[1] if len(sys.argv) > 1 else config.MODEL_SAVE_PATH
    test_file = sys.argv[2] if len(sys.argv) > 2 else None
    
    try:
        evaluator = ModelEvaluator(model_path)
        test_data = evaluator.load_test_data(test_file)
        
        print(f"ğŸ“‚ åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬\n")
        
        results = evaluator.comprehensive_evaluation(test_data)
        evaluator.print_detailed_report(results)
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

